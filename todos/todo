X - Add 'distinct' macro
     X - Parser
     X - Solver interpretation (skipping assignment candidate tuples where
         variables marked as distinct are identical)
     X - Test with more complicated instances

X - Add HornSolver.add_rule() method (which adds a rule and updates the two literal
    maps)

- Test programs with errors to find relevant cases for Parser.check()

X - Add negation macro (a or not a) and (a and not a => False)
   -> only necessary for some predicates and quick to write in a program.

X - Add equality macro (each constant is equal to itself and unequal to the rest. That makes
    the distinct macro reundant)

- Add order and equivalence operators to grammar

- Change the 'bodge' model enumaration strategy for something more complete or something complete

- Add parser for lexical items

- Define interpretation for 'sort' statements
  - sort name n
  - constant sort blah, blah, blah
  - tree name n_leaves
  
  X - read_sorts() is implemented but not tested
  - test it
  - add glue code
  
  (num name n) (tree name n) (const name ct_name) - por cada nombre ahi -

- Improve docstrings
    X - Properly documented models.py
    - Document parser
    - Document ui
    - Document ttag
    - Document tile tagger

X - Add multiple selection to tile tagger

- Testear exhaustivamente las funciones que ya están escritas

X - Dump tile properties to file

- Test a.f : fun

- Add könisberg bridge example

- agregar estructura de modulos decente

- agregarle al modelo algo para leer las sort parts que saca el parser
   X - Ta en el parser, no ta la interpretacion en models.py

- hacer que artale.py tenga artale.from_program() que parsee y aplique el pipeline bien

- agregar declaraciones de funciones

X- embedding de la negación

- Any macro
   X - draft
   X - test

X - El embedding de la negacion anda!!

- Usar un orden y una lista para contar cuantos elementos lexicos cubre cada
  diatesis:

  - Accepts(d1, e1), not in (e1, d1.list) => False
  - cons(e1, d.1list), not accepts(d1, e1) => False
  - before(e1, e2), above(e2, e1, list) => False (para romper la simetria esa)

- qué pasa si un... coso tiene dos any?
- en realidad todos los any son independientes no? total, tienen que ser todos los miembros de cada sort :)

- Add test suite with any macros

- When only functions of the sort over which a variable ranges are used, for some reason there is a bug:
  full(t : type), full(s : type), match(s.input, s.output) => match(s, t) works, but
  match(s.input : type, t.input : type) => match(s, t) doesn't. Code up to now is super ad hoc
  and full of errors like this. 
  
X - Add a set of facts to HornSolver, so you can check if p(x) by checking running "p x" in HornSolver.positive_facts.
  Maybe getting the facts from a model is better, since true facts are a property of models, and not the solver
  itself.
  -> This is not useful, since models are small arrays which can be converted to sets very quickly with set(model)

- Complete symmetry breaking (adding constraints to which constants can be right of another)
   - These will be made worse with lexical items (if there can be W words right of a given node,
     then you have WR isomorphic trees, with W right of that node, in constants cn, cn+1, cn+2...c(r-n))
   - I think node order univocally determines "which word can go where", but I'm not sure. Make
     sure the reasoning is clear and precise, and then check if it holds

- Quantify over blocks of clauses
  
  a : A, b : A, c : C {
     p(a) => q(a)
     q(b, a), r(b, c) => s(b)
     t(a, c) => y(c, a)
  }

  That requires less typing and will make embeddings faster, I think (the same number of clauses are added, but
  all n clauses are created for each set of assignments, and you don't have to allocate and cycle through assignments n times)

  - Figure out how to deal with rules using only a subset of variables (i.e. don't add the same clause on a and c |b| times)
  - Write the parser

  - In general, group rules with the same argument signature to avoid unfolding the same set of assignments more than once
     X - Group rules by argument signature
     X - Use that grouping when unfolding (that grouping above is a partition so it should be super straightforward).
     - Test whether the embedding one get with opt_unfold() are the same one gets using "regular" unfold_instance()
     X - Profile them to see if there is a significant change in elapsed time
       X - On a first cprofile run with 13 nodes, opt_unfold() took 2 seconds, and unfold_instance() took 9.
           On four other tests with 23 nodes, they both took the exact same ammount of time.
           And when testing again with 13 nodes, opt_unfold was faster, again 2 secs vs 9.
           Surprisingly, about half of the total execution time was spent executing lark.py, so I'll write
           a silly script and avoid using lark altogether.
           Printing was very expensive, as usual.
           So I'm not sure
           Apparently list comprehensions are visibly slow, so replace them with for loops.
         - Run satisfactory tests
       X - opt_unfold is much slower as instance size increases, so we're doing away with it
     
  - Limit unfolding to a subset of a sort (add a kwarg restricted_sorts={}, and if a sort s is in restricted_sorts.keys(),
    you use the contents of restricted_sorts[s] to unfold it, and not the whole sort)

- There is more (and more serious) symmetry breaking to be done! Now there are types, and those have symmetries too
  (at least in trees without a lexicon - better fix that before trying anything else!)

- Add a keyword argument sort_restrictions to HornSolver.unfold_rule(), so you can
  unfold / embed a rule only for a subset of a sort (this is useful when
  a sort can be partitioned into several independent / mutually not related
  subsets, so that rules quantifying over the sort do not waste user time and memory
  on relations that have no bearing on relevant model properties)

- Closest feature match in map tagger

- Use quadratic CYK-style tree embedding

- Find out why sometimes type rules are unfolded and sometimes they are not

  X - After printing each rule before it is unfolded, and the number of rules,
      it turns out sometimes parsing yields 91 rules, and sometimes 128. So
      the problem is not in the unfolding function, but in the parser.

      ```
      sort_declarations, rule_declarations = Parser().parse(theory_program)

      print(f"We have {len(rule_declarations)} rules")
      ```

  - Define a custom parser (cProfile showed parsing took about ten seconds, so this
    is a useful thing to do anyway)

  - Test it

  - Check if these issues cease to occur with the custom parser

- Tile Tagger UX
  X - Drag virtual nodes (if a virtual node is selected, press A to drag it around)
  X - Remove virtual nodes (select it and press d to delete it)
  - Area select (select a corner, move to a corner, then add all tiles in that interval)
  X - Autoarrange (arrange al virtual nodes as a decent parse tree)
  - Add child (select a node, press something, click something, and add it as child)
  X - Fix error in positions yielded by auto_arrange when scroll shifts are nonzero
  - What should we do when two virtual nodes overlap?
    - ok, we will now group by depth, y height, and center_x. If there are collisions,
      we will shuffle everyone around so that everyone is above their 'cosito'

- OOP
  - Whenever an object is selected, it should trigger a 'mother object' behavior telling anyone to deselect
  - Discard should be more general (every 'ux entity' should have a pyglet_object and linked entities.
    Discarding a ux entity should delete all references to that object in all linked entities and get rid
    of the entity's pyglet object decenty.

    So: link, unlink, unpyglet
    Since 'where' the reference is depends on the type of an object, uhhh, we should have a way to dynamically
    pbtain the method or property we're talking about

- Be thorough with tests
